{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "MNIST 데이터셋을 통해 `SRN(Sequence Recognition Network)`을 학습시켜보도록 하겠습니다. SRN은 CRNN의 구조와 Seq2Seq, 그리고 Attention Network을 합친 모델입니다.\n",
    "\n",
    "![Imgur](https://i.imgur.com/M11craN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf     \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST 데이터셋\n",
    "\n",
    "MNIST 데이터셋을 통해 정상적으로 동작하는지를 확인해 보도록 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - 데이터 가져오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import SerializationDataset\n",
    "\n",
    "train_set = SerializationDataset('mnist','train',\n",
    "                                 digit=5,pad_range=(3,10))\n",
    "validation_set = SerializationDataset('mnist','validation',\n",
    "                                      digit=5,pad_range=(3,10))\n",
    "test_set = SerializationDataset('mnist','test',\n",
    "                                digit=(3,8),pad_range=(3,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - 데이터 Generator 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.generator import Seq2SeqGenerator\n",
    "\n",
    "train_gen = Seq2SeqGenerator(train_set, \n",
    "                          batch_size=32)\n",
    "valid_gen = Seq2SeqGenerator(validation_set, \n",
    "                          batch_size=100, \n",
    "                          shuffle=False)\n",
    "test_gen = Seq2SeqGenerator(test_set, \n",
    "                         batch_size=500, \n",
    "                         shuffle=False)\n",
    "\n",
    "conv2text = test_gen.convert2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = train_gen[0]\n",
    "for i in range(3):\n",
    "    image = X['images'][i,:,:,0]\n",
    "    dec_input = X['decoder_inputs'][i]\n",
    "    dec_input = conv2text(dec_input)\n",
    "    \n",
    "    output = Y['output_seqs'][i]\n",
    "    output = conv2text(output)\n",
    "\n",
    "    plt.title(f\"Decoder Input : {dec_input} \\n Model Output : {output}\")\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRNN과 다른 데이터셋이 필요합니다. CRNN과 달리, Images, Decoder Input, Model Output 이렇게 총 3개의 데이터가 필요합니다. Decoder Input과 Model Output은 1번의 Time Step 만큼 차이가 납니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SRN 모델 구성하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolution Feature Extractor & Map to Sequence 부분 구성하기\n",
    "\n",
    "CRNN 부분과 동일하게 구성됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.layers import ConvFeatureExtractor, Map2Sequence\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 28\n",
    "num_classes = 10\n",
    "n_conv = 16\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# For Gray Scale Image & Dynamic width\n",
    "inputs = Input(shape=(height, None, 1),name='images')\n",
    "\n",
    "# n_conv == Convolution Filter의 갯수를 정하는 계수 F\n",
    "conv_maps = ConvFeatureExtractor(n_conv,\n",
    "                                 name='feature_extractor')(inputs)\n",
    "feature_seqs = Map2Sequence(name='map_to_sequence')(conv_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder State Vector($S_{encoder}$) 계산하기\n",
    "\n",
    "![Imgur](https://i.imgur.com/kgZLw3N.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "states_{encoder} = [H_{forward} ; H_{backward}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM\n",
    "\n",
    "class BLSTMEncoder(Layer):\n",
    "    \"\"\"\n",
    "    CRNN 중 Recurrent Layers에 해당하는 Module Class\n",
    "    Convolution Layer의 Image Feature Sequence를 Encoding하여,\n",
    "    우리가 원하는 Text Feature Sequence로 만듦\n",
    "\n",
    "    | Layer Name | #Hidden Units |\n",
    "    | ----       | ------ |\n",
    "    | Bi-LSTM1   | 256    |\n",
    "    | Bi-LSTM2   | 256    |\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_units=256, **kwargs):\n",
    "        self.n_units = n_units\n",
    "        super().__init__(**kwargs)\n",
    "        self.lstm1 = Bidirectional(LSTM(n_units, return_sequences=True))\n",
    "        self.lstm2 = Bidirectional(LSTM(n_units, return_sequences=True))\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = self.lstm1(inputs)\n",
    "        x = self.lstm2(x)\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            \"n_units\": self.n_units\n",
    "        }\n",
    "        base_config = super().get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lstm = 256\n",
    "states_encoder = BLSTMEncoder(n_units=n_lstm)(feature_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'states_encoder의 shape : {states_encoder.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention 구성하기\n",
    "\n",
    "![Imgur](https://i.imgur.com/clo5uEw.png)\n",
    "\n",
    "Attention은 우리가 필요한 정보만을 취사선택할 수 있도록 만든 모듈입니다. 글자 영상 추출기를 통해 만들어진 정보 중 Decoder에서 필요한 정보만을 취사선택할 수 있도록 만듭니다. Attention은 하나의 방법론으로, 다양한 형태로 구성할 수 있습니다. 이번에 쓰는 방법은 Luong Attention입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score 계산하기\n",
    "![Imgur](https://i.imgur.com/vNkwyPs.png)\n",
    "\n",
    "Decoder의 정보중 어떤 정보가 더 중요한 정보인지를 판단하기 위한 지표로, Score을 아래와 같이 계산합니다. 내적의 연산의 중요한 특징은, 두 벡터가 유사할수록 그 크기가 커진다는 점에 있습니다. 디코더의 벡터($S$)와 인코더의 벡터($V$)를 내적해줌으로써, 디코더와 가까운 정보에게 더 가중치를 주게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score를 Normalize 하기\n",
    "\n",
    "![Imgur](https://i.imgur.com/hWCD9fK.png)\n",
    "\n",
    "각 Time Step 별로 점수가 따로 매겨지게 됩니다. 이를 합산할 때, 그 크기가 지나치게 커지지 않도록, 전체 score의 합이 1이 되도록 표준화합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context Vector 계산하기\n",
    "\n",
    "![Imgur](https://i.imgur.com/OOvZyzv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 Encoder 정보와 Score 점수를 곱해서 나온 값이 바로 Context Vector가 됩니다. 이 정보는 Encoder의 정보 중 필요한 정보만을 추출한 정보가 됩니다. 이 정보를 바탕으로 분류기에 넣으면 우리가 원하는 철자 정보를 얻을 수 있게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Softmax\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotAttention(Layer):\n",
    "    \"\"\" General Dot-Product Attention Network (Luong, 2015)\n",
    "\n",
    "    * n_state :\n",
    "       if n_state is None, Dot-Product Attention(s_t * h_i)\n",
    "       if n_state is number, general Dot-Product Attention(s_t * W_a * h_i)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_state=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_state = n_state\n",
    "        if isinstance(self.n_state, int):\n",
    "            self.key_dense = Dense(self.n_state)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        states_encoder = inputs[0]\n",
    "        states_decoder = inputs[1]\n",
    "\n",
    "        # (0) adjust the size of encoder state to the size of decoder state\n",
    "        if isinstance(self.n_state, int):\n",
    "            key_vector = self.key_dense(states_encoder)\n",
    "        else:\n",
    "            key_vector = states_encoder\n",
    "\n",
    "        # (1) Calculate Score\n",
    "        expanded_states_encoder = key_vector[:, None, ...]\n",
    "        # >>> (batch size, 1, length of encoder sequence, num hidden)\n",
    "        expanded_states_decoder = states_decoder[..., None, :]\n",
    "        # >>> (batch size, length of decoder sequence, 1, num hidden)\n",
    "        score = K.sum(expanded_states_encoder * expanded_states_decoder,\n",
    "                      axis=-1)\n",
    "        # >>> (batch size, length of decoder input, length of encoder input)\n",
    "        # (2) Normalize score\n",
    "        attention = Softmax(axis=-1, name='attention')(score)\n",
    "\n",
    "        # (3) Calculate Context Vector\n",
    "        value_vector = states_encoder[:, None, ...] # Key Vector와 Value Vector을 다르게 둚\n",
    "        context = K.sum(value_vector * attention[..., None], axis=2)\n",
    "        # >>> (batch size, length of decoder input, num hidden)\n",
    "\n",
    "        return context, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder 구성하기\n",
    "\n",
    "어텐션에 Query를 던질 Decoder을 구현해보도록 하겠습니다. 글자를 임베딩하는 Embedding Layer와 GRUCell을 이용하도록 하겠습니다.\n",
    "\n",
    "![Imgur](https://i.imgur.com/f0jLCf5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Gray Scale Image & Dynamic width\n",
    "n_embed = 3\n",
    "decoder_inputs = Input(shape=(None,),name='decoder_inputs')\n",
    "\n",
    "embedding_layer = Embedding(num_classes+1, n_embed)\n",
    "embeded_decoder_inputs = embedding_layer(decoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_layer = GRU(n_lstm*2, \n",
    "                name='decoder_gru', \n",
    "                return_sequences=True)\n",
    "states_decoder = gru_layer(embeded_decoder_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Imgur](https://i.imgur.com/yArrBKh.png)\n",
    "\n",
    "우리는 초기 state를 넣는 인자를 따로 만들었습니다.<br>\n",
    "이후에 inference Logic을 짤 때, Decoder Logic에서 필요하므로 추가하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention Layer 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Softmax\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def call(self, inputs, **kwargs):\n",
    "        states_encoder = inputs[0]\n",
    "        states_decoder = inputs[1]\n",
    "        \n",
    "        # (1) Calculate Score\n",
    "        expanded_states_encoder = states_encoder[:,None,...] \n",
    "        # >>> (batch size, 1, length of encoder sequence, num hidden)\n",
    "        expanded_states_decoder = states_decoder[...,None,:] \n",
    "        # >>> (batch size, length of decoder sequence, 1, num hidden)            \n",
    "        score = K.sum(expanded_states_encoder * expanded_states_decoder,\n",
    "                          axis=-1)\n",
    "        # >>> (batch size, length of decoder input, length of encoder input)\n",
    "        \n",
    "        # (2) Normalize score\n",
    "        attention = Softmax(axis=-1, name='attention')(score)\n",
    "        \n",
    "        # (3) Calculate Context Vector\n",
    "        context = K.sum(expanded_states_encoder * attention[...,None], axis=2)\n",
    "        # >>> (batch size, length of decoder input, num hidden)\n",
    "        \n",
    "        return context, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotattend = DotAttention()\n",
    "\n",
    "context, attention = dotattend([states_encoder, states_decoder])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prediction with Softmax\n",
    "\n",
    "![Imgur](https://i.imgur.com/ihz1Hpq.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, concatenate\n",
    "\n",
    "n_clf = 256\n",
    "\n",
    "clf1_layer = Dense(n_clf, activation='relu')\n",
    "clf2_layer = Dense(num_classes+1, \n",
    "                   activation='softmax',\n",
    "                   name='output_seqs')\n",
    "\n",
    "concat_output = concatenate([context, states_decoder],\n",
    "                            name='concat_output')\n",
    "fc_outputs = clf1_layer(concat_output)\n",
    "predictions = clf2_layer(fc_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전체 모델 구성하고 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "height = 28\n",
    "num_classes = 10\n",
    "n_conv = 16 # the number of Convolution filter\n",
    "n_state = 128 # the number of BLSM units\n",
    "n_embed = 10 # the size of embedding vector\n",
    "n_clf = 128 # the number of units in classifier Dense layer\n",
    "\n",
    "# Image Encoder\n",
    "inputs = Input(shape=(height, None, 1),name='images')\n",
    "conv_maps = ConvFeatureExtractor(n_conv,\n",
    "                                 name='feature_extractor')(inputs)\n",
    "feature_seqs = Map2Sequence(name='map_to_sequence')(conv_maps)\n",
    "states_encoder = BLSTMEncoder(n_units=n_state)(feature_seqs)    \n",
    "\n",
    "# Embedding Layer\n",
    "decoder_inputs = Input(shape=(None,), name='decoder_inputs')\n",
    "embedding_layer = Embedding(num_classes+1, n_embed)\n",
    "embedding_target = embedding_layer(decoder_inputs)\n",
    "\n",
    "# Text Decoder\n",
    "decoder_state_inputs = Input(shape=(n_state*2,), name='decoder_state')\n",
    "gru_layer = GRU(n_state*2, \n",
    "                name='decoder_gru', \n",
    "                return_sequences=True)\n",
    "states_decoder = gru_layer(embedding_target,\n",
    "                           initial_state=decoder_state_inputs)\n",
    "\n",
    "# Attention Layer\n",
    "dotattend = DotAttention()\n",
    "context, attention = dotattend([states_encoder, states_decoder])\n",
    "\n",
    "# Classifier Layer\n",
    "clf1_layer = Dense(n_clf, activation='relu')\n",
    "clf2_layer = Dense(num_classes+1, activation='softmax',name='output_seqs')\n",
    "\n",
    "concat_output = concatenate([context, states_decoder], name='concat_output')\n",
    "fc_outputs = clf1_layer(concat_output)\n",
    "predictions = clf2_layer(fc_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 추론 모델과 학습 모델 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# For training\n",
    "trainer = Model([inputs, \n",
    "                 decoder_inputs,\n",
    "                 decoder_state_inputs], \n",
    "                predictions, name='trainer')\n",
    "\n",
    "# For Inference\n",
    "# - (1) Encoder\n",
    "encoder = Model(inputs, states_encoder, \n",
    "                name='encoder')\n",
    "\n",
    "# - (2) Decoder\n",
    "states_encoder_input = Input((None, n_state*2), \n",
    "                             name='states_encoder_input')\n",
    "\n",
    "context, attention = dotattend([states_encoder_input, states_decoder])\n",
    "concat_output = concatenate([context, states_decoder], axis=-1, \n",
    "                            name='concat_output')\n",
    "fc_outputs = clf1_layer(concat_output)\n",
    "predictions = clf2_layer(fc_outputs)\n",
    "\n",
    "decoder = Model([states_encoder_input, decoder_inputs, decoder_state_inputs], \n",
    "                [states_decoder, predictions], name='decoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 모델 Compile하기\n",
    "\n",
    "학습할 모델에 대한 Loss Function와 optimizer를 결정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def masking_sparse_categorical_crossentropy(mask_value):\n",
    "    \"\"\"\n",
    "    Runs sparse Categorical Crossentropy Loss Algorithm on each batch element Without Masking Value\n",
    "\n",
    "    :param mask_value: masking value for preventing Back Propagation\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, K.floatx())\n",
    "        mask = K.equal(y_true, mask_value)\n",
    "        mask = 1 - K.cast(mask, K.floatx())\n",
    "        y_true = y_true * mask\n",
    "\n",
    "        loss = K.sparse_categorical_crossentropy(y_true, y_pred) * mask\n",
    "        return K.sum(loss) / K.sum(mask)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델의 출력값의 shape과 입력값의 shape가 다르면, 아래와 같이 `target_tensors`을 명시해 주어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = Input(shape=(None,), dtype=tf.int32)\n",
    "\n",
    "trainer.compile(Adam(lr=1e-3),\n",
    "                loss={\"output_seqs\":masking_sparse_categorical_crossentropy(-1)},\n",
    "                target_tensors=[y_true])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "callbacks =[]\n",
    "rlrop = ReduceLROnPlateau(\n",
    "    factor=0.5, patience=5, \n",
    "    min_lr=1e-6, verbose=1)\n",
    "callbacks.append(rlrop)\n",
    "\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = Seq2SeqGenerator(train_set, \n",
    "                          batch_size=32,\n",
    "                          state_size=n_state*2)\n",
    "valid_gen = Seq2SeqGenerator(validation_set, \n",
    "                             batch_size=100,\n",
    "                             shuffle=False,\n",
    "                             state_size=n_state*2)                            \n",
    "test_gen = Seq2SeqGenerator(test_set, \n",
    "                            batch_size=500, \n",
    "                            shuffle=False,\n",
    "                            state_size=n_state*2)                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = trainer.fit_generator(train_gen,\n",
    "                             epochs=epochs,\n",
    "                             validation_data=valid_gen,\n",
    "                             callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 평가하기\n",
    "\n",
    "재귀적으로 EOS Token이 나올 때까지 Decoder의 과정을 반복합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_gen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b4d24e354a6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mEOS_TOKEN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_gen\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Target image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_gen' is not defined"
     ]
    }
   ],
   "source": [
    "EOS_TOKEN = 10\n",
    "\n",
    "X,_ = test_gen[0]\n",
    "\n",
    "# Target image \n",
    "target_images = X['images'][:10]\n",
    "\n",
    "# Encoder 결과 계산\n",
    "states_encoder_ = encoder.predict(target_images)\n",
    "\n",
    "# Decoder\n",
    "batch_size = target_images.shape[0]\n",
    "\n",
    "prev_inputs = np.ones((batch_size,1)) * EOS_TOKEN\n",
    "prev_states = np.zeros((batch_size, 512))\n",
    "\n",
    "result = prev_inputs.copy()\n",
    "while True:\n",
    "    states_decoder_, predictions_ = decoder.predict({\n",
    "        \"states_encoder_input\" : states_encoder_,\n",
    "        \"decoder_inputs\": prev_inputs,\n",
    "        \"decoder_state\": prev_states        \n",
    "    })\n",
    "    prev_states = states_decoder_[:,-1,:]\n",
    "    prev_inputs = np.argmax(predictions_,axis=-1)\n",
    "    \n",
    "    if np.all(prev_inputs == EOS_TOKEN):\n",
    "        break\n",
    "    result = np.concatenate([result,prev_inputs],axis=-1)\n",
    "result = result[:,1:].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, seq in zip(target_images,result):\n",
    "    plt.title(seq)\n",
    "    plt.imshow(image[:,:,0])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
